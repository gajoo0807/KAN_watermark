{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iPv5Nnr8iGf"
      },
      "source": [
        "# Implementation of a KAN for regression\n",
        "In this notebook I implement a Kolmogorov-Arnold Network (KAN) for the use of regression and compare it against a neural network of a similar architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRMpDTp_cVt5"
      },
      "source": [
        "## Initialisations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrVEGBzEcUQc",
        "outputId": "0ffd8432-65a5-4d4f-9c76-5e682ff9a386"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from kan import *\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RE1svm9cXkX"
      },
      "source": [
        "## Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMwDaT0icUJD",
        "outputId": "313d1117-4c58-4ecf-a68b-bf1116a80d34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16512/16512 [00:01<00:00, 12528.79it/s]\n",
            "100%|██████████| 4128/4128 [00:00<00:00, 12460.39it/s]\n"
          ]
        }
      ],
      "source": [
        "def load_calhous_dataset():\n",
        "    # Load California housing dataset\n",
        "    calhous = fetch_california_housing()\n",
        "    data = calhous.data\n",
        "    target = calhous.target\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    data = scaler.fit_transform(data)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
        "    target_tensor = torch.tensor(target, dtype=torch.float32)\n",
        "\n",
        "    # Split dataset into train and test sets\n",
        "    train_data, test_data, train_target, test_target = train_test_split(data_tensor, target_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create data loaders (optional, if you want to batch and shuffle the data)\n",
        "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_data, train_target), batch_size=1, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_data, test_target), batch_size=1, shuffle=False)\n",
        "\n",
        "    train_inputs = torch.empty(0, 8, device=device)\n",
        "    train_labels = torch.empty(0, dtype=torch.long, device=device)\n",
        "    test_inputs = torch.empty(0, 8, device=device)\n",
        "    test_labels = torch.empty(0, dtype=torch.long, device=device)\n",
        "\n",
        "    # Concatenate all data into a single tensor on the specified device\n",
        "    for data, labels in tqdm(train_loader):\n",
        "        train_inputs = torch.cat((train_inputs, data.to(device)), dim=0)\n",
        "        train_labels = torch.cat((train_labels, labels.to(device)), dim=0)\n",
        "\n",
        "    for data, labels in tqdm(test_loader):\n",
        "        test_inputs = torch.cat((test_inputs, data.to(device)), dim=0)\n",
        "        test_labels = torch.cat((test_labels, labels.to(device)), dim=0)\n",
        "\n",
        "    dataset = {}\n",
        "    dataset['train_input'] = train_inputs\n",
        "    dataset['test_input'] = test_inputs\n",
        "    dataset['train_label'] = train_labels.reshape(-1, 1)\n",
        "    dataset['test_label'] = test_labels.reshape(-1, 1)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "calhous_dataset = load_calhous_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDBtdgGocUHM",
        "outputId": "c5c54a1a-0367-4d0b-988f-d8040ed25a1b"
      },
      "outputs": [],
      "source": [
        "print(\"Train data shape: {}\".format(calhous_dataset['train_input'].shape))\n",
        "print(\"Train target shape: {}\".format(calhous_dataset['train_label'].shape))\n",
        "print(\"Test data shape: {}\".format(calhous_dataset['test_input'].shape))\n",
        "print(\"Test target shape: {}\".format(calhous_dataset['test_label'].shape))\n",
        "print(\"====================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ERosp1iM17"
      },
      "source": [
        "## Creating and Training the KAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xkjQDBTnNFHw"
      },
      "outputs": [],
      "source": [
        "image_folder = 'video_img'\n",
        "model_index = 5\n",
        "\n",
        "model = KAN(width=[8, 3, 1], grid=3, k=3, seed=model_index, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.normal(0,1,size=(100,8)).to(device)\n",
        "model(x) # do a forward pass to obtain model.acts\n",
        "# model.get_range(0,0,0)\n",
        "print(f\"{model.act_fun[0].grid[0].data=}\") # Check the initial grid, size = grid + 1\n",
        "print(f\"{model.act_fun[0].coef[0].data=}\")  # Check the initial coef, size = grid + k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhNKU6T1iLWe",
        "outputId": "c1050905-cf4e-428f-ff3f-80d82944b34d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train loss: 5.57e-01 | test loss: 5.80e-01 | reg: 3.62e+00 : 100%|█| 100/100 [04:23<00:00,  2.63s/it\n"
          ]
        }
      ],
      "source": [
        "def train_mse():\n",
        "    with torch.no_grad():\n",
        "        predictions = model(calhous_dataset['train_input'])\n",
        "        mse = torch.nn.functional.mse_loss(predictions, calhous_dataset['train_label'])\n",
        "    return mse\n",
        "\n",
        "def test_mse():\n",
        "    with torch.no_grad():\n",
        "        predictions = model(calhous_dataset['test_input'])\n",
        "        mse = torch.nn.functional.mse_loss(predictions, calhous_dataset['test_label'])\n",
        "    return mse\n",
        "\n",
        "# results = model.train(calhous_dataset, opt=\"LBFGS\", device=device, metrics=(train_mse, test_mse),\n",
        "#                       loss_fn=torch.nn.MSELoss(), steps=50, lamb=0.01, lamb_entropy=2., save_fig=True, img_folder=image_folder)\n",
        "\n",
        "results = model.train(calhous_dataset, opt=\"LBFGS\", device=device, metrics=(train_mse, test_mse),\n",
        "                      loss_fn=torch.nn.MSELoss(), steps=100, lamb=0.01, lamb_entropy=2., save_fig=True, img_folder=image_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8hG6q3zujPG",
        "outputId": "2a99f4b0-896f-47b2-a980-cdfb6a314f11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train MSE: 0.31013, Test MSE: 0.33585\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train MSE: {results['train_mse'][-1]:.5f}, Test MSE: {results['test_mse'][-1]:.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_inference_time():\n",
        "    import random\n",
        "    import time\n",
        "\n",
        "    # 从测试集中随机选择一个样本\n",
        "    index = random.randint(0, len(calhous_dataset['test_input']) - 1)\n",
        "    input_data = calhous_dataset['test_input'][index].unsqueeze(0)\n",
        "    label = calhous_dataset['test_label'][index]\n",
        "\n",
        "    # 将输入数据传入模型进行预测\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_data)\n",
        "    end = time.time()\n",
        "    # print(f\"Inference time: {end - start} seconds\")\n",
        "    return end - start\n",
        "\n",
        "def print_act():\n",
        "    for i in model.acts_scale:\n",
        "        print(i)\n",
        "        print('\\n\\n')\n",
        "\n",
        "time = []\n",
        "for i in range(11):\n",
        "    time.append(detect_inference_time())\n",
        "print(time)\n",
        "print(sum(time[1:]) / (len(time)-1))\n",
        "# print_act()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mode = \"auto\" # \"manual\"\n",
        "\n",
        "if mode == \"manual\":\n",
        "    # manual mode\n",
        "    model.fix_symbolic(0,0,0,'sin');\n",
        "    model.fix_symbolic(0,1,0,'x^2');\n",
        "    model.fix_symbolic(1,0,0,'exp');\n",
        "elif mode == \"auto\":\n",
        "    # automatic mode\n",
        "    lib = ['x','x^2','x^3','x^4','exp','log','sqrt','tanh','sin','abs']\n",
        "    model.auto_symbolic(lib=lib)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "ll0aE1WW9FEK",
        "outputId": "ad5f27d7-a28b-4dde-9feb-e7ad93a1510b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x400 with 28 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPbmkBTBNSDe",
        "outputId": "dc4c7c69-c45a-4b56-d086-59b0a2f102f9"
      },
      "outputs": [],
      "source": [
        "import moviepy.video.io.ImageSequenceClip\n",
        "\n",
        "video_name='video'\n",
        "fps=10\n",
        "\n",
        "fps = fps\n",
        "files = os.listdir(image_folder)\n",
        "train_index = []\n",
        "for file in files:\n",
        "    if file[0].isdigit() and file.endswith('.jpg'):\n",
        "        train_index.append(int(file[:-4]))\n",
        "\n",
        "train_index = np.sort(train_index)\n",
        "\n",
        "image_files = [image_folder+'/'+str(train_index[index])+'.jpg' for index in train_index]\n",
        "\n",
        "clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(image_files, fps=fps)\n",
        "clip.write_videofile(video_name+'.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import imageio\n",
        "def generate_train_gif(l, i, j, step, model_index=0):\n",
        "    # 设置图片所在的文件夹和输出的 GIF 文件名\n",
        "    folder = f'train_process/neuron_{l}_{i}_{j}'\n",
        "    output_gif = f'neuron_{l}_{i}_{j}.gif'\n",
        "    save_folder = f'fingerprint/model_{model_index}'\n",
        "\n",
        "    # 收集文件夹中所有的 PNG 文件名\n",
        "    images = []\n",
        "    for i in range(step):  # 假设有 50 张图片，编号从 0 到 49\n",
        "        img_path = os.path.join(folder, f'step_{i}.png')\n",
        "        if os.path.exists(img_path):  # 确认图片存在\n",
        "            images.append(imageio.imread(img_path))\n",
        "\n",
        "    # 将收集到的图片列表制作成 GIF\n",
        "    imageio.mimsave(os.path.join(save_folder, output_gif), images, fps=10)  # fps 是每秒帧数\n",
        "index = 1\n",
        "for i in range(8):\n",
        "    for j in range(3):\n",
        "        generate_train_gif(0,i,j,100, index)\n",
        "for i in range(3):\n",
        "    generate_train_gif(1, i, 0, 100, index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "save this model to ./model_ckpt/model_5.pth\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model.save_ckpt(f'model_{model_index}.pth') # Save model to ./model_ckpt/test.pth\n",
        "model.save_curve(f\"private_key/model_{model_index}\", 0, 0, 0)\n",
        "# model.load_ckpt('test.pth') # Load model from ./model_ckpt/test.pth\n",
        "\n",
        "# 可用作finetune attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prune_neuron():\n",
        "    # print(model.acts_scale)\n",
        "    # for i in model.acts_scale:\n",
        "    #     print(torch.mean(i))\n",
        "    #     print(i.type())\n",
        "    model.prune(threshold=1e-4)\n",
        "    model.plot()\n",
        "\n",
        "    \n",
        "\n",
        "    # acts_scale_mean = torch.mean(model.acts_scale)\n",
        "    # print(acts_scale_mean)\n",
        "\n",
        "prune_neuron()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.prune()\n",
        "model.plot(mask=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBD58aME1Rvd"
      },
      "source": [
        "# Implementation of a Neural Network for comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeOsPZiB9MSS"
      },
      "source": [
        "## Misc Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxOUQTTiuAja"
      },
      "outputs": [],
      "source": [
        "class CalHousNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CalHousNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(8, 3)  # 8 inputs to 5 hidden nodes\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(3, 1)  # 5 hidden nodes to 3 outputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=25):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the specified device\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_count = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the specified device\n",
        "            outputs = model(inputs)\n",
        "            loss = torch.nn.functional.mse_loss(outputs, labels, reduction='sum')\n",
        "            total_loss += loss.item()\n",
        "            total_count += labels.size(0)\n",
        "\n",
        "    # Calculate the average MSE over all batches\n",
        "    average_mse = total_loss / total_count\n",
        "    print(f'Mean Squared Error: {average_mse}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-99o3TR1uN1"
      },
      "outputs": [],
      "source": [
        "def load_calhous_dataset():\n",
        "    # Load California housing dataset\n",
        "    calhous = fetch_california_housing()\n",
        "    data = calhous.data\n",
        "    target = calhous.target\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    data = scaler.fit_transform(data)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
        "    target_tensor = torch.tensor(target, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "    # Split dataset into train and test sets\n",
        "    train_data, test_data, train_target, test_target = train_test_split(data_tensor, target_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create data loaders (optional, if you want to batch and shuffle the data)\n",
        "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_data, train_target), batch_size=256, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_data, test_target), batch_size=256, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "train_loader, test_loader = load_calhous_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJYeBGm9vj28",
        "outputId": "1b10911b-f937-4f21-8cc7-586b74cac384"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(train_loader)\n",
        "data, labels = next(dataiter)\n",
        "print(\"Train data shape: {}\".format(data.shape))\n",
        "print(\"Train target shape: {}\".format(labels.shape))\n",
        "dataiter = iter(test_loader)\n",
        "data, labels = next(dataiter)\n",
        "print(\"Test data shape: {}\".format(data.shape))\n",
        "print(\"Test target shape: {}\".format(labels.shape))\n",
        "print(\"====================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg219SyH9NrO"
      },
      "source": [
        "## Train and Evaluate the NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbsyfVGd1cX1",
        "outputId": "e5b8d8b7-abda-4f53-fa54-f38c83f3b561"
      },
      "outputs": [],
      "source": [
        "model = CalHousNet().to(device)\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
        "\n",
        "train_model(model, train_loader, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw5MP7SSwmOq",
        "outputId": "89d40fbe-1dcb-486b-8ac7-df08c4cc5f05"
      },
      "outputs": [],
      "source": [
        "test_model(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LRMpDTp_cVt5",
        "7RE1svm9cXkX"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
